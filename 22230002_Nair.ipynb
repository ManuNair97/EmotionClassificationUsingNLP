{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I965pXSHOX5"
      },
      "source": [
        "For this assignment, we will explore Twitter Emotion Classification. The goal is the identify the primary emotion expressed in a tweet. Consider the following tweets:\n",
        "```\n",
        "Tweet 1: @NationalGallery @ThePoldarkian I have always loved this painting.\n",
        "Tweet 2: '@tateliverpool #BobandRoberta: I am angry more artists that have a profile are not speaking up #foundationcourses.'\n",
        "``` \n",
        "\n",
        "How would you describe the emotions in `Tweet 1` vs `Tweet 2`? `Tweet 1` expresses enjoyment and happiness, while `Tweet 2` directly expresses anger. For this assignment, we will be working with the SMILE Twitter Emotion Dataset ([Wang et al. 2016](https://ceur-ws.org/Vol-1619/paper3.pdf)). At a high level, our goal is to develop different models (rule-based, machine learning, and deep learning), which can be used to identify the emotion of a tweet. You will be required to clean and preprocess the data, generate features for classification, train various models, and evaluate the models. \n",
        "\n",
        "\n",
        "*Submission Details*\n",
        "Please complete all the tasks in ‚ÄúAssignment 1.ipynb‚Äù and upload your submission as a Python notebook on Blackboard with the filename ‚ÄúStudentID_Lastname.ipynb‚Äù. Assignment 1 will be due by 11:59 PM GMT Monday February 20th, 2023.  \n",
        "\n",
        "*Grading Policy*\n",
        "\n",
        "Assignment 1 is graded and will be worth 25% of your overall grade. This assignment is worth a total of 50 points distributed over the tasks below. \n",
        "Please note that this is an individual assignment and you must not work with other students to complete this assessment. Any copying from other students, from student exercises from previous years, and any internet resources will not be tolerated. Plagiarised assignments will receive zero marks and the students who commit this act will be reported. \n",
        "\n",
        "Feel free to reach out to the TAs and instructors if you have any questions.\n",
        "\n",
        "Before you get started, run the cell below to download the dataset into memory and a few relevant libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONlRrW-SFy8u",
        "outputId": "e5eb9f7f-a5ef-4c69-a595-2ac52028c9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mavEEVoPHYS1",
        "outputId": "1583fa17-c2d1-428b-f8c0-af2c98edcc64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-27 23:20:02--  https://figshare.com/ndownloader/files/4988956\n",
            "Resolving figshare.com (figshare.com)... 54.217.34.18, 34.252.222.205, 2a05:d018:1f4:d003:825f:f38:d5f1:5837, ...\n",
            "Connecting to figshare.com (figshare.com)|54.217.34.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/4988956/smileannotationsfinal.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230227/eu-west-1/s3/aws4_request&X-Amz-Date=20230227T232002Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=1a1bca37a5e58bf8a922d7383809be4af326c86536a43dce0df8c82f07f57d8f [following]\n",
            "--2023-02-27 23:20:02--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/4988956/smileannotationsfinal.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230227/eu-west-1/s3/aws4_request&X-Amz-Date=20230227T232002Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=1a1bca37a5e58bf8a922d7383809be4af326c86536a43dce0df8c82f07f57d8f\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.53.194, 52.218.24.163, 52.92.1.16, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.53.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 429669 (420K) [binary/octet-stream]\n",
            "Saving to: ‚Äòdata.csv‚Äô\n",
            "\n",
            "data.csv            100%[===================>] 419.60K  1.19MB/s    in 0.3s    \n",
            "\n",
            "2023-02-27 23:20:03 (1.19 MB/s) - ‚Äòdata.csv‚Äô saved [429669/429669]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (2.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ],
      "source": [
        "!wget -O data.csv \"https://figshare.com/ndownloader/files/4988956\"\n",
        "!pip install emoji\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsMEwTiYHZ3J"
      },
      "source": [
        "## Task 1. Data Cleaning, Preprocessing, and splitting [15 points]\n",
        "The `data` environment contains the SMILE dataset loaded into a pandas dataframe object. Our dataset has three columns: id, tweet, and label. The `tweet` column contains the raw scraped tweet and the `label` column contains the annotated emotion category. Each tweet is labelled with one of the following emotion labels:\n",
        "- 'nocode', 'not-relevant' \n",
        "- 'happy', 'happy|surprise', 'happy|sad'\n",
        "- 'angry', 'disgust|angry', 'disgust' \n",
        "- 'sad', 'sad|disgust', 'sad|disgust|angry' \n",
        "- 'surprise'\n",
        "\n",
        "### Task 1a. Label Consolidation [ 3 points]\n",
        "As we can see above the annotated categories are complex. Several tweets express complex emotions like (e.g. 'happy|sad') or multiple emotions (e.g. 'sad|disgust|angry'). The first things we need to do is clean up our dataset by removing complex examples and consolidating others so that we have a clean set of emotions to predict. \n",
        "\n",
        "For Task 1a., write code which does the following:\n",
        "1. Drops all rows which have the label \"happy|sad\", \"happy|surprise\", 'sad|disgust|angry', and 'sad|angry'.\n",
        "2. Re-label 'nocode' and 'not-relevant' as 'no-emotion'.\n",
        "3. Re-label 'disgust|angry' and 'disgust' as 'angry'.\n",
        "4. Re-label 'sad|disgust' as 'sad'.\n",
        "\n",
        "Your updated `data' dataframe should have 3,062 rows and 5 label categories (no-emotion, happy, angry, sad, and surprise).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKSm3ZcNHabO",
        "outputId": "f58fcb04-b6cf-49b3-c44d-f877076fb52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[4mLABEL COUNTS BEFORE PREPROCESSING\n",
            "\u001b[0m\n",
            "nocode               1572\n",
            "happy                1137\n",
            "not-relevant          214\n",
            "angry                  57\n",
            "surprise               35\n",
            "sad                    32\n",
            "happy|surprise         11\n",
            "happy|sad               9\n",
            "disgust|angry           7\n",
            "disgust                 6\n",
            "sad|disgust             2\n",
            "sad|angry               2\n",
            "sad|disgust|angry       1\n",
            "Name: label, dtype: int64\n",
            "\n",
            "\u001b[1m\u001b[4mLABEL COUNTS AFTER PREPROCESSING\n",
            "\u001b[0m\n",
            "nocode           1572\n",
            "happy            1137\n",
            "not-relevant      214\n",
            "angry              57\n",
            "surprise           35\n",
            "sad                32\n",
            "disgust|angry       7\n",
            "disgust             6\n",
            "sad|disgust         2\n",
            "Name: label, dtype: int64\n",
            "\n",
            "\u001b[1m\u001b[4mLABEL COUNTS AFTER RE-LABELLING\n",
            "\u001b[0m\n",
            "no-emotion    1786\n",
            "happy         1137\n",
            "angry           70\n",
            "surprise        35\n",
            "sad             34\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"data.csv\", names=[\"id\", \"tweet\", \"label\"])\n",
        "\n",
        "#checking the current values for labels (current counts)\n",
        "print('\\033[1m'+ '\\033[4m' + \"LABEL COUNTS BEFORE PREPROCESSING\\n\" + '\\033[0m')\n",
        "print(data.label.value_counts())\n",
        "\n",
        "#removing the rows which has labels 'nocode', 'happy', 'not-relevant', 'angry','surprise', 'sad', \"disgust|angry\", 'disgust', \"sad|disgust\"\n",
        "data = data[data.label.isin(['nocode', 'happy', 'not-relevant', 'angry','surprise', 'sad', \"disgust|angry\", 'disgust', \"sad|disgust\"])]\n",
        "print('\\n\\033[1m'+ '\\033[4m' + \"LABEL COUNTS AFTER PREPROCESSING\\n\" + '\\033[0m')\n",
        "print(data.label.value_counts())\n",
        "\n",
        "#source: https://datatofish.com/replace-values-pandas-dataframe/\n",
        "#relabelling 'nocode' and 'not-relevant' as 'no-emotion'.\n",
        "data['label'] = data['label'].replace(['nocode', 'not-relevant'], 'no-emotion')\n",
        "\n",
        "#relabelling 'disgust|angry' and 'disgust' as 'angry'\n",
        "data['label'] = data['label'].replace(['disgust|angry', 'disgust'], 'angry')\n",
        "\n",
        "#relabelling 'sad|disgust' as 'sad'.\n",
        "data['label'] = data['label'].replace(['sad|disgust'], 'sad')\n",
        "\n",
        "print('\\n\\033[1m'+ '\\033[4m' + \"LABEL COUNTS AFTER RE-LABELLING\\n\" + '\\033[0m')\n",
        "print(data.label.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCkpdlS6HdZx"
      },
      "source": [
        "### Task 1a Tests \n",
        "Run the cell below to evaluate your code. To get full credit for this task, your code must pass all tests. Any alteration of the testing code will automatically result in 0 points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPvK4r4rHhly",
        "outputId": "edc4c474-0b65-45ac-e836-ab73fcf99c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label test: True\n",
            "Label check: True\n",
            "Angry example count: True\n",
            "Happy example count: True\n",
            "No-Emotion example count: True\n",
            "Sad example count: True\n",
            "Surprise example count: True\n"
          ]
        }
      ],
      "source": [
        "# Test 1. Data should have 5 unique labels.\n",
        "print(f\"Unique label test: {len(data['label'].unique()) == 5}\")\n",
        "\n",
        "# Test 2. Data labels must be: angry, happy, no-emotion, sad, and surprise\n",
        "labels = [\"angry\", \"happy\", \"no-emotion\", \"sad\", \"surprise\"]\n",
        "print(f\"Label check: { set(data['label'].unique()).difference(labels) == set() }\")\n",
        "\n",
        "# Test 3. Check example counts per label\n",
        "print(f\"Angry example count: {len(data[data['label']=='angry']) == 70}\")\n",
        "print(f\"Happy example count: {len(data[data['label']=='happy']) == 1137}\")\n",
        "print(f\"No-Emotion example count: {len(data[data['label']=='no-emotion']) == 1786}\")\n",
        "print(f\"Sad example count: {len(data[data['label']=='sad']) == 34}\")\n",
        "print(f\"Surprise example count: {len(data[data['label']=='surprise']) == 35}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg-m3lROHex9"
      },
      "source": [
        "### Task 1b. Tweet Cleaning and Processing [10 points]\n",
        "Raw tweets are noisy. Consider the example below: \n",
        "```\n",
        "'@tateliverpool #BobandRoberta: I am angry more artists that have a profile are not speaking up #foundationcourses. üò†'\n",
        "```\n",
        "The mention @tateliverpool and hashtag #BobandRoberta are extra noise that don't directly help with understanding the emotion of the text. The accompanying emoji can be useful but needs to be decoded to it text form :angry: first. \n",
        "\n",
        "For this task you will fill complete the `preprocess_tweet` function below with the following preprocessing steps:\n",
        "1. Lower case all text\n",
        "2. De-emoji the text\n",
        "3. Remove all hashtags, mentions, and urls\n",
        "4. Remove all non-alphabet characters except the followng punctuations: period, exclamation mark, and question mark\n",
        "\n",
        "Hints: \n",
        "- For step 2 (de-emoji), consider using the python [emoji](https://carpedm20.github.io/emoji/docs/) library. The `emoji.demojize` method will convert all emojis to plain text. The `emoji` library is installed in cell [52].\n",
        "- Follow the processing steps in order. For example calling nltk's word_tokenize before removing hashtags and mentions will end up creating seperate tokens for @ and # and cause problems.\n",
        "\n",
        "To get full credit for this task, the Test 1b must pass. Only modify the  cell containing the `preprocess_tweet` function and do not alter the testing code block. \n",
        "\n",
        "After you are satisfied with your code, run the tests. code to ensure your function works as expected. This cell will also create a new column called `cleaned_tweet` and apply the `preproces_tweet` function to all the examples in the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJHQXAizHjoE",
        "outputId": "75f64097-5691-4e80-d145-705f1c10391a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am angry more artists that have a profile are not speaking up! angryface\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import emoji \n",
        "import re #makes it easier for us to preprocess the text as we are removing many unwanted characters from the tweet\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
        "\n",
        "def preprocess_tweet(tweet: str) -> str:\n",
        "    \n",
        "    #code referred from the re module of python\n",
        "    \n",
        "    #code which will convert all the characters in the tweet to lowercase\n",
        "    tweet = tweet.strip().lower()\n",
        "    \n",
        "    #code to convert all emojis to plain text.\n",
        "    tweet = emoji.demojize(tweet)\n",
        "    \n",
        "    #code to remove mentions\n",
        "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
        "    \n",
        "    #code to remove hashtags\n",
        "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
        "    \n",
        "    #code to remove any http links from the tweet\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "    \n",
        "    #code to remove ' from the tweet\n",
        "    tweet = re.sub(\"'\", \"\", tweet)\n",
        "    \n",
        "    #code to remove any non alphabetical character from a text using their ascii value\n",
        "    tweet = ''.join([c for c in tweet if ord(c) < 128])\n",
        "    \n",
        "    #code to remove any digits\n",
        "    tweet = re.sub(r'\\d+', '', tweet)\n",
        "    \n",
        "    #code to remove tweets that has words starting with & like &amp\n",
        "    tweet = re.sub(\"&[A-Za-z0-9_]+\",\"\", tweet)\n",
        "    \n",
        "    #since we have removed all the hashtags and mentions, we can tokenize the tweet\n",
        "    tweet = nltk.word_tokenize(tweet)\n",
        "    tweet = filter(lambda x: x not in [\":\", \";\", \"&\", \"(\", \")\", \"<\", \">\", \"{\", \"}\", \"[\", \"]\", \",\", \"$\", \"%\", \"^\", \"*\", \"-\", \"+\", \"-\", \"/\"], tweet)\n",
        "    tweet = [tweet.replace(\"_\", \"\") for tweet in tweet]\n",
        "    #tweet = [tweet.replace(\"'\", \"\") for tweet in tweet]\n",
        "    \n",
        "    tweet = TreebankWordDetokenizer().detokenize(tweet)\n",
        "    \n",
        "    #code to remove the trailing white spaces for the start and end of the tweet\n",
        "    tweet = tweet.strip()\n",
        "    \n",
        "    return tweet \n",
        "\n",
        "\n",
        "test_tweet = \"'@tateliverpool #BobandRoberta: I am angry more artists that have a profile are not speaking up! #foundationcourses üò†'\"\n",
        "print(preprocess_tweet(test_tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ2IAIoVHmd5"
      },
      "source": [
        "### Task 1b Test\n",
        "Run the cell below to evaluate your code. To get full credit for this task, your code must pass all tests. Any alteration of the testing code will automatically result in 0 points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb71pAhcHodX",
        "outputId": "e500627e-134e-48d6-d91c-27bf2639caec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1b: True\n"
          ]
        }
      ],
      "source": [
        "# Do NOT modify the code below. \n",
        "# Create new column with cleaned tweets. We will use this for the subsequent tasks\n",
        "data[\"cleaned_tweet\"] = data[\"tweet\"].apply(preprocess_tweet)\n",
        "\n",
        "# Test 1b \n",
        "test_tweet = \"'@tateliverpool #BobandRoberta: I am angry more artists that have a profile are not speaking up! #foundationcourses üò†'\"\n",
        "clean_tweet = \"i am angry more artists that have a profile are not speaking up! angryface\"\n",
        "print(f\"Test 1b: {preprocess_tweet(test_tweet) == clean_tweet}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoBlBQQFHpmG"
      },
      "source": [
        "### Task 1c. Generating Evaluation Splits [2 points]\n",
        "Finally, we need to split our data into a train, validation, and test set. We will split the data using a 60-20-20 split, where 60% of our data is used for training, 20% for validation, and 20% for testing. As the dataset is heaviliy imbalanced, make sure you stratify the dataset to ensure that the label distributions across the three splits are roughly equal. \n",
        "\n",
        "Store your splits in the variables `train`, `val`, and `test` respectively. \n",
        "\n",
        "Hints:\n",
        "- Use the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function for this task. You'll have to call it twice to get the validation split. \n",
        "- Set the random state so the sampling can be reproduced (we use 2023 for our random state)\n",
        "- Use the `stratify` parameter to ensure representative label distributions across the splits. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Dgc7zu0gHqm6",
        "outputId": "d0ef953c-dc9b-4a0e-eae8-35b1512db0ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      id                                              tweet  \\\n",
              "2161  610362800683270144  Last week: #CanAltay @kurusehir @tateliverpool...   \n",
              "1566  614359996822876161  See all the photos from Wednesday's #Defeating...   \n",
              "1674  612391451817701380  .How To Make A Turquoise Goblet. via @britishm...   \n",
              "2037  611153439817605120  Time is running out to catch New Rhythms @kett...   \n",
              "2946  612948937960521728  @NationalGallery /Ôºé„ÄÅ„ÄÄ„ÄÄÔΩÄ\\ | Ôºõ„ÄÄ„ÄÄ„ÄÄ„ÄÄ Ôºõ \\ (`'„ÉºÔºå,„Éº'`...   \n",
              "...                  ...                                                ...   \n",
              "3027  613017736088715265  Stunning Viking silver 'thistle' brooch (AD 90...   \n",
              "2324  613031427807006720  #LeonardoDaVinci The Virgin and... http://t.co...   \n",
              "1176  614704511836377088  Take #AWalkontheWildSide today with a visit to...   \n",
              "1793  611892587721572352  What a treat RT @FitzMuseum_UK: Exhibited for ...   \n",
              "419   613268388748652544  ¬£13m Cezanne painting will leave Cambridge's @...   \n",
              "\n",
              "           label                                      cleaned_tweet  \n",
              "2161  no-emotion                                          last week  \n",
              "1566  no-emotion        see all the photos from wednesdays event at  \n",
              "1674  no-emotion              .how to make a turquoise goblet . via  \n",
              "2037  no-emotion  time is running out to catch new rhythms as we...  \n",
              "2946  no-emotion  /\\ | \\ ` ` /\\  ` \"\" ` \"\" `  \\ ` \",-\" ` |\\--/| ...  \n",
              "...          ...                                                ...  \n",
              "3027  no-emotion  stunning viking silver thistle brooch ad s at ...  \n",
              "2324  no-emotion                                 the virgin and ...  \n",
              "1176       happy  take today with a visit to the art gallery . f...  \n",
              "1793       happy  what a treat rt exhibited for the first time i...  \n",
              "419   no-emotion  m cezanne painting will leave cambridges unles...  \n",
              "\n",
              "[1837 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f50afbb-d13b-4463-a09f-a1ee7ccb123e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2161</th>\n",
              "      <td>610362800683270144</td>\n",
              "      <td>Last week: #CanAltay @kurusehir @tateliverpool...</td>\n",
              "      <td>no-emotion</td>\n",
              "      <td>last week</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1566</th>\n",
              "      <td>614359996822876161</td>\n",
              "      <td>See all the photos from Wednesday's #Defeating...</td>\n",
              "      <td>no-emotion</td>\n",
              "      <td>see all the photos from wednesdays event at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1674</th>\n",
              "      <td>612391451817701380</td>\n",
              "      <td>.How To Make A Turquoise Goblet. via @britishm...</td>\n",
              "      <td>no-emotion</td>\n",
              "      <td>.how to make a turquoise goblet . via</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2037</th>\n",
              "      <td>611153439817605120</td>\n",
              "      <td>Time is running out to catch New Rhythms @kett...</td>\n",
              "      <td>no-emotion</td>\n",
              "      <td>time is running out to catch new rhythms as we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2946</th>\n",
              "      <td>612948937960521728</td>\n",
              "      <td>@NationalGallery /Ôºé„ÄÅ„ÄÄ„ÄÄÔΩÄ\\ | Ôºõ„ÄÄ„ÄÄ„ÄÄ„ÄÄ Ôºõ \\ (`'„ÉºÔºå,„Éº'`...</td>\n",
              "      <td>no-emotion</td>\n",
              "      <td>/\\ | \\ ` ` /\\  ` \"\" ` \"\" `  \\ ` \",-\" ` |\\--/| ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3027</th>\n",
              "      <td>613017736088715265</td>\n",
              "      <td>Stunning Viking silver 'thistle' brooch (AD 90...</td>\n",
              "      <td>no-emotion</td>\n",
              "      <td>stunning viking silver thistle brooch ad s at ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2324</th>\n",
              "      <td>613031427807006720</td>\n",
              "      <td>#LeonardoDaVinci The Virgin and... http://t.co...</td>\n",
              "      <td>no-emotion</td>\n",
              "      <td>the virgin and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>614704511836377088</td>\n",
              "      <td>Take #AWalkontheWildSide today with a visit to...</td>\n",
              "      <td>happy</td>\n",
              "      <td>take today with a visit to the art gallery . f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1793</th>\n",
              "      <td>611892587721572352</td>\n",
              "      <td>What a treat RT @FitzMuseum_UK: Exhibited for ...</td>\n",
              "      <td>happy</td>\n",
              "      <td>what a treat rt exhibited for the first time i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>613268388748652544</td>\n",
              "      <td>¬£13m Cezanne painting will leave Cambridge's @...</td>\n",
              "      <td>no-emotion</td>\n",
              "      <td>m cezanne painting will leave cambridges unles...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1837 rows √ó 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f50afbb-d13b-4463-a09f-a1ee7ccb123e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f50afbb-d13b-4463-a09f-a1ee7ccb123e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f50afbb-d13b-4463-a09f-a1ee7ccb123e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 300
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and test sets\n",
        "train, val = train_test_split(data, test_size=0.4, stratify=data.label, random_state=2023)\n",
        "\n",
        "# Split data into val and test sets\n",
        "val, test = train_test_split(val, test_size=0.5, stratify=val.label, random_state=2023)\n",
        "\n",
        "#train.shape,test.shape,val.shape\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAu45L3QHutm"
      },
      "source": [
        "## Task 2: Naive Baseline Using a Rule-based Classifier [10 points]\n",
        "\n",
        "Now that we have a dataset, let's work on developing some solutions for emotion classification. We'll start with implementing a simple rule-based classifier which will also serve as our naive baseline. Emotive language (e.g. awesome, feel great, super happy) can be a strong signal as to the overall emotion being by the tweet. For each emotion in our label space (happy, surprised, sad, angry) we will generate a set of words and phrases that are often associated with that emotion. At classification time, the classifier will calculate a score based on the overlap between the words in the tweet and the emotive words and phrases for each of the emotions. The emotion label with the highest overlap will be selected as the prediction and if there is no match the \"no-emotion\" label will be predicted. We can break the implementation of this rules-based classifier into three steps:\n",
        "1. Emotive language extraction from train examples \n",
        "2. Developing a scoring algorithm\n",
        "3. Building the end-to-end classification flow \n",
        "\n",
        "### Task 2a. Emotive Language Extraction [4 points] \n",
        "For this task you will generate a set of unigrams and bigrams that will be used to predict each of the labels. Using the training data you will need to extract all the unique unigrams and bigrams associated with each label (excluding no-emotion). Then you should ensure that the extracted terms for each emotion label do not appear in the other lists. In the real world, you would then manually curate the generated lists to ensure that associated words were useful and emotive. For the assignment, you won't be required to further curate the generated lists.\n",
        "\n",
        "Once you've identified the appropiate terms, save them as lists stored in the following environment variables: `happy_words`, `surprised_words`, `sad_words`,and `angry_words`. To get full credit for this section, ensure all 2a Tests pass. \n",
        "\n",
        "Hints\n",
        "- We suggest you use Python's [set methods](https://realpython.com/python-sets/) for this task.\n",
        "- NLTK has a function for extracting [ngrams](https://www.nltk.org/api/nltk.util.html?highlight=ngrams#nltk.util.ngrams). This function expects a list of tokens as input and will output tuples which you'll need to reconvert into strings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "ZCIw1gA18m3L"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "from typing import List\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# 1. Extract all terms associated with each label\n",
        "def extract_words(examples: List[str]) -> List[str]:\n",
        "    \n",
        "    \"\"\"\n",
        "    Given a list of tweets, return back the unigrams and bigrams found\n",
        "    across all the tweets. \n",
        "    \"\"\"\n",
        "    extracted_words = set()\n",
        "    \n",
        "    for example in examples:\n",
        "\n",
        "        example = preprocess_tweet(example)\n",
        "        \n",
        "        #For each of the tweet, create unigrams and bigrams.\n",
        "        #Referred lab notes from previous year\n",
        "        unigrams = set(nltk.word_tokenize(example))\n",
        "        bigrams = set(ngrams(nltk.word_tokenize(example), 2))\n",
        "        \n",
        "        #Update the retrieved word set to include the unigrams and bigrams.\n",
        "        extracted_words.update(unigrams)\n",
        "        extracted_words.update(bigrams)\n",
        "    \n",
        "    return extracted_words\n",
        "    \n",
        "# Words to be taken out for each emotion label\n",
        "happy_words = extract_words(tweet for tweet in train[train['label'] == 'happy']['cleaned_tweet'].tolist())\n",
        "sad_words = extract_words(tweet for tweet in train[train['label'] == 'sad']['cleaned_tweet'].tolist())\n",
        "angry_words = extract_words(tweet for tweet in train[train['label'] == 'angry']['cleaned_tweet'].tolist())\n",
        "surprise_words = extract_words(tweet for tweet in train[train['label'] == 'surprise']['cleaned_tweet'].tolist())\n",
        "\n",
        "# Eliminate redundant terms from each emotion list.\n",
        "# Referred from https://www.geeksforgeeks.org/python-set-difference/\n",
        "happy_words = set(happy_words) - set(sad_words) - set(angry_words) - set(surprise_words)\n",
        "sad_words = set(sad_words) - set(happy_words) - set(angry_words) - set(surprise_words)\n",
        "angry_words = set(angry_words) - set(happy_words) - set(sad_words) - set(surprise_words)\n",
        "surprise_words = set(surprise_words) - set(happy_words) - set(sad_words) - set(angry_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe6NX0STHyHk"
      },
      "source": [
        "### Task 2a Tests\n",
        "Run the cell below to evaluate your code. To get full credit for this task, your code must pass all tests. Any alteration of the testing code will automatically result in 0 points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD4VhSqDHzQa",
        "outputId": "bb1ee5fd-36f3-4cb8-b417-5ea2fc09a4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking sets are not empty: \n",
            "Happy words count: 7056, True\n",
            "Sad words count: 416, True\n",
            "Angry words count: 758, True\n",
            "Surprise words count: 358, True\n",
            "\n",
            "Checking sets are all disjoint:\n",
            "Happy words disjoint: True\n",
            "Sad words disjoint: True\n",
            "Angry words disjoint: True\n",
            "Surprise words disjoint: True\n"
          ]
        }
      ],
      "source": [
        "# Check sets are non-empty\n",
        "print(\"Checking sets are not empty: \")\n",
        "print(f\"Happy words count: {len(happy_words)}, {len(happy_words) > 0}\")\n",
        "print(f\"Sad words count: {len(sad_words)}, {len(sad_words) > 0}\")\n",
        "print(f\"Angry words count: {len(angry_words)}, {len(angry_words) > 0}\")\n",
        "print(f\"Surprise words count: {len(surprise_words)}, {len(surprise_words) > 0}\")\n",
        "\n",
        "# Checks sets are disjoint \n",
        "union1 = sad_words.union(angry_words, surprise_words)\n",
        "union2 = happy_words.union(surprise_words, angry_words) \n",
        "union3 = surprise_words.union(happy_words, sad_words)\n",
        "union4 = angry_words.union(happy_words, sad_words) \n",
        "\n",
        "print(\"\\nChecking sets are all disjoint:\")\n",
        "print(f\"Happy words disjoint: {happy_words.isdisjoint(union1)}\")\n",
        "print(f\"Sad words disjoint: {sad_words.isdisjoint(union2)}\")\n",
        "print(f\"Angry words disjoint: {angry_words.isdisjoint(union3)}\")\n",
        "print(f\"Surprise words disjoint: {surprise_words.isdisjoint(union4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ6vlgcHH0-G"
      },
      "source": [
        "### Task 2b. Scoring using set overlaps [2 points]\n",
        "\n",
        "Next we will implement to scoring algorithm. Our score will simply be the count of overlapping terms between tweet text and emotive terms. For this task, finish implementing the code below. To get full credit, ensure Test 2b. is successful. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN4E5de4H1zm",
        "outputId": "0912ca3c-669e-4fb4-847c-a60263592259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1: True\n",
            "Test 2: True\n",
            "Test 3: True\n"
          ]
        }
      ],
      "source": [
        "sample_words = {'cat', 'hat', 'mat', 'bowling', 'bat'}\n",
        "sample_tweet1 = \"that cat is super cool sitting on the mat\" \n",
        "sample_tweet2 = \"the man in the bowling hat sat on the cat\"\n",
        "sample_tweet3 = \"the quick brown fox jumped over the lazy dog\"\n",
        "\n",
        "#This function accepts a tweet as a string and returns the number of emotive words in the tweet. Return type is int\n",
        "def score_tweet(tweet: str, emotive_words: set) -> int:\n",
        "\n",
        "    words = preprocess_tweet(tweet).split()\n",
        "    return len(emotive_words.intersection(words))\n",
        "\n",
        "print(f\"Test 1: {score_tweet(sample_tweet1, sample_words) == 2}\")\n",
        "print(f\"Test 2: {score_tweet(sample_tweet2, sample_words) == 3}\")\n",
        "print(f\"Test 3: {score_tweet(sample_tweet3, sample_words) == 0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6qORzgxH3X4"
      },
      "source": [
        "### 2c. Rule-based classification [4 points] \n",
        "Let put together our rules-based classfication system. Fill out the logic in the `simple_clf`. Given a tweet, `simple_clf` will generate the overlap score\n",
        "for each of emotion labels and return the emotion label with the highest score. If there is no match amongst the emotions, the classifier will return 'no-emotion'.\n",
        "\n",
        "To get full credit for this section, your average F1 score most be greater than 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "E8IEuXP0H46K"
      },
      "outputs": [],
      "source": [
        "def simple_clf(tweet: str) -> str:\n",
        "    \n",
        "    \"\"\"\n",
        "    Given a tweet, calculate all the emotion overlap scores.\n",
        "    Return the emotion label which has the largest score. If\n",
        "    overlap score is 0, return no-emotion. \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    tweet_words = set(preprocess_tweet(tweet).split())\n",
        "    \n",
        "    \"\"\"\n",
        "    Each word set representing an emotion is tallied for the number of words that overlap.\n",
        "    It is stored in a dictionary called scores\n",
        "    \n",
        "    Example: When a tweet has been preprocessed, the set of unique words in that tweet is contained in tweet_words, and \n",
        "    the set of unique words connected to the emotion \"happy\" is contained in happy_words. We obtain the set of words that \n",
        "    are shared by both sets by taking the intersection of these two sets.\n",
        "    \n",
        "    \"\"\"\n",
        "    scores = {\n",
        "        \n",
        "        \"happy\": len(tweet_words.intersection(happy_words)),\n",
        "        \"sad\": len(tweet_words.intersection(sad_words)),\n",
        "        \"angry\": len(tweet_words.intersection(angry_words)),\n",
        "        \"surprise\": len(tweet_words.intersection(surprise_words))\n",
        "    }\n",
        "    \n",
        "    \n",
        "    #Return the emotion with the highest score, or 'no-emotion' if all scores are 0\n",
        "    \n",
        "    \"\"\"\n",
        "    The largest value in the scores dictionary is located, and the matching key is returned to identify the emotion \n",
        "    with the highest overlap score.\n",
        "    Returns the string \"no-emotion\" if all the scores are 0.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    max_score = max(scores.values())\n",
        "    \n",
        "    if max_score == 0:\n",
        "        return \"no-emotion\"\n",
        "    else:\n",
        "        for emotion, score in scores.items():\n",
        "            if score == max_score:\n",
        "                return emotion\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YbxbkhtH-nC"
      },
      "source": [
        "After finishing the above section, let's evaluate our how model did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUE4GNIbH8dp",
        "outputId": "38d88eb8-a27b-4c68-9e59-c637fbbfa454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.05      0.21      0.08        14\n",
            "       happy       0.46      0.48      0.47       228\n",
            "  no-emotion       0.86      0.08      0.15       357\n",
            "         sad       0.00      0.00      0.00         7\n",
            "    surprise       0.03      1.00      0.05         7\n",
            "\n",
            "    accuracy                           0.24       613\n",
            "   macro avg       0.28      0.36      0.15       613\n",
            "weighted avg       0.67      0.24      0.27       613\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "preds = test[\"cleaned_tweet\"].apply(simple_clf)\n",
        "print(classification_report(test[\"label\"], preds)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jlp3Qx1IC68"
      },
      "source": [
        "## Task 3. Machine learning w/ grammar augmented features [10 points]\n",
        "\n",
        "Now that we have a naive baseline, let's build a more sophisticated solution using machine learning. Up to this point, we have only considered the words in the tweet as our primary features. The rules-based approach is a very simple bag-of-words classifier. Can we improve performance if we provide some additional linguistic knowledge?\n",
        "\n",
        "For Task 3 you will do the following:\n",
        "- Generate part-of-speech features our tweets\n",
        "- Train two different machine learning classifiers, one with linguistic features and one without\n",
        "- Evaluate the trained models on the test set\n",
        "\n",
        "### Task 3a. Grammar Augmented Feature Generation [3 points]\n",
        "For this task, we will be generating part-of-speech tags for each token in our tweet. Additionally we'll lemmatize the text as well. We will directly include the POS information by appending the tag to the lemma of word itself. For example:\n",
        "```\n",
        "Raw Tweet: I am very angry with the increased prices.\n",
        "POS Augmented Tweet: I-PRP be-VBP very-RB angry-JJ with-IN the-DT increase-VBN price-NNS .-.\n",
        "```\n",
        "\n",
        "Complete the `generate_pos_features` using the Spacy library. Once you have an implementation that works, we'll update the `train` and `test` dataframes with a new column called `tweet_with_pos` which contains the output of the `generate_pos_features` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaVd_orT8m3N",
        "outputId": "5db4d79d-284d-47ed-d62f-8baaecd9e736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-27 23:20:18.941330: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-27 23:20:18.941514: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-27 23:20:18.941539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.22.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JAQ7V7IGIFCz",
        "outputId": "897da516-689c-4a81-e9f9-7021cf170dea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I-PRP hate-VBP action-NN movie-NNS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 307
        }
      ],
      "source": [
        "import spacy \n",
        "from tqdm.notebook import tqdm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def generate_pos_features(tweet: str) -> str:\n",
        "    \n",
        "    \"\"\"\n",
        "    Given a tweet, return the lemmatized tweet augmented\n",
        "    with POS tags.\n",
        "    E.g.:\n",
        "    Input: \"cats are super cool.\"\n",
        "    output: \"cat-NNS be-VBP super-RB cool-JJ .-.\"\n",
        "\n",
        "    \"\"\"\n",
        "    #Tokenize the tweet and tag the appropriate parts of speech.\n",
        "    doc = nlp(tweet)\n",
        "    \n",
        "    #Create an empty string to store the pos_tweet\n",
        "    pos_tweet = \"\"\n",
        "    \n",
        "    # Since we have tokenized the tweet using nlp, iterate over each token in the doc\n",
        "    \n",
        "    for token in doc:\n",
        "        \n",
        "        #Extract the token's lemma (lemma_) and part-of-speech tag(tag_)\n",
        "        pos_tag = token.tag_\n",
        "        lemma = token.lemma_\n",
        "        \n",
        "        #To the POS-augmented tweet, add the lemma and POS tag.\n",
        "        #Adding a space between the lemma and POS\n",
        "        pos_tweet += f\"{lemma}-{pos_tag} \"\n",
        "    \n",
        "    #Return the POS-augmented tweet after removing the trailing whitespace using strip()\n",
        "    return pos_tweet.strip()\n",
        "\n",
        "sample_tweet = \"I hate action movies\"\n",
        "generate_pos_features(sample_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "H5N5gJCCIJ3z"
      },
      "outputs": [],
      "source": [
        "# Once you have the code working above run this cell.\n",
        "train[\"tweet_with_pos\"] = train[\"cleaned_tweet\"].apply(generate_pos_features)\n",
        "test[\"tweet_with_pos\"] = test[\"cleaned_tweet\"].apply(generate_pos_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agVdLZKQILMi"
      },
      "source": [
        "### Task 3a Tests\n",
        "Run the cell below to evaluate your code. To get full credit for this task, your code must pass all tests. Any alteration of the testing code will automatically result in 0 points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lBgpYPbIMjG",
        "outputId": "03bea57c-4981-40d4-b7e5-5dbaf3d7e9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1: True\n",
            "Test 2: True\n",
            "Test 3: True\n"
          ]
        }
      ],
      "source": [
        "sample_texts = [\n",
        "    (\"i am super angry\", \"I-PRP be-VBP super-RB angry-JJ\"),\n",
        "    (\"That movie was great\", \"that-DT movie-NN be-VBD great-JJ\"),\n",
        "    (\"I hate action movies\", \"I-PRP hate-VBP action-NN movie-NNS\")\n",
        "]\n",
        "for i, text in enumerate(sample_texts):\n",
        "  print(f\"Test {i+1}: {generate_pos_features(text[0]) == text[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAoYl4h0IOgb"
      },
      "source": [
        "### Task 3b. Model Training [5 points]\n",
        "Next we will train two seperate RandomForest Classifier models. For this task you will generate two sets of input features using the `TfidfVectorizer`. We generate Tfidf statistic on the`cleaned_tweet` and the `tweet_with_pos` columns. \n",
        "\n",
        "Once you've generated your features, train two different Random Forest classifiers with the generated features and generate the predictions on the test set for each classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "cE2NgOBnIQPg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier as rf\n",
        "\n",
        "# Code referred from NLP Assignment 2 from Semester-1\n",
        "# Generate TF-IDF features for cleaned_tweet\n",
        "\n",
        "tfidf_vectorizer_for_cleaned_tweet = TfidfVectorizer()\n",
        "train_tfidf_clean = tfidf_vectorizer_for_cleaned_tweet.fit_transform(train['cleaned_tweet'])\n",
        "test_tfidf_clean = tfidf_vectorizer_for_cleaned_tweet.transform(test['cleaned_tweet'])\n",
        "\n",
        "# Generate TF-IDF features for tweet_with_pos\n",
        "tfidf_vectorizer_for_pos = TfidfVectorizer()\n",
        "train_tfidf_pos = tfidf_vectorizer_for_pos.fit_transform(train['tweet_with_pos'])\n",
        "test_tfidf_pos = tfidf_vectorizer_for_pos.transform(test['tweet_with_pos'])\n",
        "\n",
        "# Train a random forest classifier on cleaned_tweet\n",
        "rf_clean = rf()\n",
        "rf_clean.fit(train_tfidf_clean, train['label'])\n",
        "clean_pred = rf_clean.predict(test_tfidf_clean)\n",
        "\n",
        "# Train a random forest classifier on tweet_with_pos\n",
        "rf_pos = rf()\n",
        "rf_pos.fit(train_tfidf_pos, train['label'])\n",
        "pos_pred = rf_pos.predict(test_tfidf_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUL5ZI9wITW3"
      },
      "source": [
        "### Task 3c. [2 points]\n",
        "Generate classification reports for both models. Print the reports below. In a few sentences (no more than 100 words) explain which features were the most effective and why you think that's the case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B8ep5lhIUiT",
        "outputId": "3d0405d5-44cd-409e-fa0c-8b9af0fe6c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[4mClassification report for TFIDF features\n",
            "\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       1.00      0.07      0.13        14\n",
            "       happy       0.84      0.68      0.75       228\n",
            "  no-emotion       0.78      0.94      0.85       357\n",
            "         sad       0.00      0.00      0.00         7\n",
            "    surprise       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.80       613\n",
            "   macro avg       0.52      0.34      0.35       613\n",
            "weighted avg       0.79      0.80      0.78       613\n",
            "\n",
            "\u001b[1m\u001b[4mClassification report for TFIDF w/ POS features\n",
            "\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.50      0.07      0.12        14\n",
            "       happy       0.77      0.68      0.72       228\n",
            "  no-emotion       0.78      0.90      0.83       357\n",
            "         sad       0.00      0.00      0.00         7\n",
            "    surprise       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.78       613\n",
            "   macro avg       0.41      0.33      0.34       613\n",
            "weighted avg       0.75      0.78      0.76       613\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Classification Report for Tfidf features\n",
        "print('\\033[1m'+ '\\033[4m' + \"Classification report for TFIDF features\\n\" + '\\033[0m')\n",
        "print(classification_report(test['label'], clean_pred))\n",
        "\n",
        "# Classfication Report for POS features \n",
        "print('\\033[1m'+ '\\033[4m' + \"Classification report for TFIDF w/ POS features\\n\" + '\\033[0m')\n",
        "print(classification_report(test['label'], pos_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n015Byn3IWhW"
      },
      "source": [
        "### Your evaluation here.\n",
        "The TFIDF features without POS tagging produced greater precision, recall, and f1-score values for each class, which suggests that they were more successful based on the classification reports. This can be due to the lack of additional information that the POS features offered that was helpful for classification. Also, it's possible that the TFIDF features were more indicative of the total text content and helped the model better capture the specifics of each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKY4ti3_IYvA"
      },
      "source": [
        "## Task 4. Transfer Learning with DistilBERT [10 points]\n",
        "\n",
        "For this task you will finetune a pretrained language model (DistilBERT) using the huggingface `transformers` library. For this task you will need to:\n",
        "- Encode the tweets using the BERT tokenizer\n",
        "- Create pytorch datasets for for the train, val and test datasets\n",
        "- Finetune the distilbert model for 5 epochs\n",
        "- Extract predictions from the model's output logits and convert them into the emotion labels.\n",
        "- Generate a classification report on the predictions.\n",
        "\n",
        "Ensure you are running the notebook in Google Colab with the gpu runtime enabled for this section."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing all the required libraries at once**"
      ],
      "metadata": {
        "id": "kQAZTdZZF1y3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "Lof5sghu8m3P"
      },
      "outputs": [],
      "source": [
        "# Referred lab notes\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!pip install transformers >> NULL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label encoding**"
      ],
      "metadata": {
        "id": "gdaEvCxGF9hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_labels(df):\n",
        "    df['label'] = df['label'].replace({'happy':0,'sad':1,'no-emotion':2,'angry':3,'surprise':4})\n",
        "    \n",
        "replace_labels(train)\n",
        "replace_labels(val)\n",
        "replace_labels(test)"
      ],
      "metadata": {
        "id": "1ErVWW9aEDUW"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The AutoTokenizer.from_pretrained method will automatically load the associated tokenizer and vocabulary associated with the transformer model.**\n"
      ],
      "metadata": {
        "id": "h2Lj5vFGGGQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohBquMyHEQv1",
        "outputId": "054553f5-8cd7-4334-e216-b94023f95344"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Custom Class for DistilBert Inputs\n",
        "class SentimentAnalysisDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, encodings: dict):  \n",
        "        self.encodings = encodings\n",
        "  \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        e = {k: v[idx] for k,v in self.encodings.items()}\n",
        "        return e "
      ],
      "metadata": {
        "id": "HRweCkbfEbfF"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing Steps**"
      ],
      "metadata": {
        "id": "BnuvI20ZKrtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The train, val, and test datasets are created by calling create_dataset with the corresponding data frames.\n",
        "This code defines a create_dataset function that takes a DataFrame and returns a PyTorch dataset.\n",
        "\n",
        "\"\"\"\n",
        "def create_dataset(df):\n",
        "    # Encode inputs\n",
        "    encodings = tokenizer(\n",
        "        df[\"cleaned_tweet\"].tolist(), \n",
        "        padding=True,           # pad all inputs to max length\n",
        "        max_length=48,          # Bert max is 512, we choose 128 due to compute limitations\n",
        "        return_tensors=\"pt\",    # Return format pytorch tensor\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Add labels to inputs\n",
        "    labels = torch.tensor(df[\"label\"].tolist())\n",
        "    encodings[\"label\"] = labels\n",
        "\n",
        "    dataset = SentimentAnalysisDataset(encodings)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Train dataset\n",
        "train_dataset = create_dataset(train)\n",
        "\n",
        "# Validation dataset\n",
        "val_dataset = create_dataset(val)\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = create_dataset(test)\n",
        "test_new = test[\"label\"].tolist()\n"
      ],
      "metadata": {
        "id": "Z0gMENFqHYUP"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained Model"
      ],
      "metadata": {
        "id": "XR2ukkuGK8lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    lr_scheduler_type='cosine',\n",
        "    per_device_train_batch_size = 112,\n",
        "    per_device_eval_batch_size = 112, \n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP8-gaGNEuTv",
        "outputId": "0bc84c49-c528-48e1-b692-c1f4d32c7484"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "S5SvRNEfFH8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "preds = trainer.predict(test_dataset)\n",
        "import numpy as np\n",
        "# Find the index of the maximum value along each row of the preds array\n",
        "pred_new = np.argmax(preds[0], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J39zdj0fFdi-",
        "outputId": "e94e8ec2-5896-41e1-8c4a-254e589851ed"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1837\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 112\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 112\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 85\n",
            "  Number of trainable parameters = 66957317\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:33, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.627504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.505930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.492759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.485753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.483202</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 612\n",
            "  Batch size = 112\n",
            "Saving model checkpoint to ./results/checkpoint-17\n",
            "Configuration saved in ./results/checkpoint-17/config.json\n",
            "Model weights saved in ./results/checkpoint-17/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 612\n",
            "  Batch size = 112\n",
            "Saving model checkpoint to ./results/checkpoint-34\n",
            "Configuration saved in ./results/checkpoint-34/config.json\n",
            "Model weights saved in ./results/checkpoint-34/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 612\n",
            "  Batch size = 112\n",
            "Saving model checkpoint to ./results/checkpoint-51\n",
            "Configuration saved in ./results/checkpoint-51/config.json\n",
            "Model weights saved in ./results/checkpoint-51/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 612\n",
            "  Batch size = 112\n",
            "Saving model checkpoint to ./results/checkpoint-68\n",
            "Configuration saved in ./results/checkpoint-68/config.json\n",
            "Model weights saved in ./results/checkpoint-68/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 612\n",
            "  Batch size = 112\n",
            "Saving model checkpoint to ./results/checkpoint-85\n",
            "Configuration saved in ./results/checkpoint-85/config.json\n",
            "Model weights saved in ./results/checkpoint-85/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Prediction *****\n",
            "  Num examples = 613\n",
            "  Batch size = 112\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting prediction out of the model**"
      ],
      "metadata": {
        "id": "HMKqjdwPLExn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_new, pred_new))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zyyYWQzGzw1",
        "outputId": "b45afe9c-2f9a-42b6-ef24-0ee4f19a6ed8"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       228\n",
            "           1       0.00      0.00      0.00         7\n",
            "           2       0.87      0.92      0.89       357\n",
            "           3       0.00      0.00      0.00        14\n",
            "           4       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.86       613\n",
            "   macro avg       0.34      0.36      0.35       613\n",
            "weighted avg       0.82      0.86      0.84       613\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsOhvhDOImmH"
      },
      "source": [
        "## Task 5. Model Recommendation [5 points]\n",
        "In a paragraph (no more than 250 words) answer the following questions:\n",
        "1. Which of the implemented models would you recommend and why? \n",
        "2. Compare the metrics for each models implemted (Rules-Based, Machine Learning w/ POS features, and DistilBERT). What are the pros and con for each model (consider performance both macro performance and label specifc metrics and the computational requirements). "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DistilBERT model exceeds the other two models in terms of accuracy, precision, recall, and F1-score, according to the published classification reports. Consequently, for this classification problem, I propose utilizing the DistilBERT model as it gives an accuracy of 86% which is better than the other two models.\n",
        "\n",
        "The advantages and disadvantages of each model are contrasted below:\n",
        "\n",
        "**1. Model Based on Rules**\n",
        "\n",
        "Positives: \n",
        "\n",
        "*   Simple to comprehend and interpret. \n",
        "*   Little computational resources are needed.\n",
        "\n",
        "Cons:\n",
        "\n",
        "*  Results might not generalize well to new data.\n",
        "\n",
        "\n",
        "**2. Machine learning features for POS**\n",
        "\n",
        "Positives: \n",
        "\n",
        "*   Captures more intricate data patterns than rules-based models.\n",
        "*   The performance of the model can be enhanced by POS features.\n",
        "\n",
        "Cons:\n",
        "\n",
        "*   If the model is overfit or the feature engineering is poor, it may still not generalize effectively to new data.\n",
        "\n",
        "\n",
        "**3. DistilBERT**\n",
        "\n",
        "Positives: \n",
        "\n",
        "*   Data can be captured with complex relationships and patterns without the need for feature engineering.\n",
        "\n",
        "*   Able to generalize well to novel data and circumstances.\n",
        "\n",
        "Cons:\n",
        "\n",
        "*   Training takes a lot of time and processing power.\n",
        "*   It could be challenging to comprehend how the model functions and makes decisions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zpKnXCqsF8Xs"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}